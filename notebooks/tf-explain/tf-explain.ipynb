{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Explain technique of explaining neural networks\n",
    "From https://github.com/sicara/tf-explain\n",
    "\n",
    "Before you start remember to create virtual environment. Type in your shell: \n",
    "\n",
    "```virtualenv venv -p python3.6```\n",
    "\n",
    "Create new kernel using your virtualenv (https://anbasile.github.io/posts/2017-06-25-jupyter-venv/) and turn it on in the notebook **(Top bar > Kernel > Change kernel > ...).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-explain in ./venv/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in ./venv/lib/python3.6/site-packages (from tf-explain) (4.2.0.32)\n",
      "Requirement already satisfied: numpy>=1.11.3 in ./venv/lib/python3.6/site-packages (from opencv-python>=4.1.0.25->tf-explain) (1.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-explain\n",
    "pip install tensorflow==2.1.0\n",
    "pip install pillow\n",
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all libraries and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import sys\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2 as cv\n",
    "from tf_explain.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure that your keras backend is switch to 'tensorflow' and your image data format is 'channels_last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception = tf.keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list of image paths and class indexes of images from ImageNet\n",
    "You have to organise your images like: *./images/xyz/* where xyz is class index from ImageNet (e.g. ./images/96/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = glob('./images/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/96/toucan.jpg\n",
      "./images/96/toucanet.jpg\n",
      "./images/96/toucanet_2.jpg\n",
      "./images/71/real_scorpion.jpg\n",
      "./images/71/scorpion.jpg\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "\n",
    "for class_path in classes_list:\n",
    "    class_index = class_path.split('/')[-1]\n",
    "    path_for_glob = class_path + '/*.jpg'\n",
    "    images_paths = glob(path_for_glob)\n",
    "    \n",
    "    for image in images_paths:\n",
    "        \n",
    "        print(image)\n",
    "        data_list.append((image,int(class_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create diffrent types of explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start explainer\n",
    "explainer_grad_cam = GradCAM()\n",
    "explainer_vanilla = VanillaGradients()\n",
    "explainer_smooth = SmoothGrad()\n",
    "explainer_occlusion = OcclusionSensitivity()\n",
    "explainer_int_grad = IntegratedGradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load images from path and start explaining with all five techniques and save the result images\n",
    "Make sure, you've chosen right model (inception or vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/96/toucan.jpg\n",
      "toucan\n",
      "./images/96/toucanet.jpg\n",
      "toucanet\n",
      "./images/96/toucanet_2.jpg\n",
      "toucanet_2\n",
      "./images/71/real_scorpion.jpg\n",
      "real_scorpion\n",
      "./images/71/scorpion.jpg\n",
      "scorpion\n"
     ]
    }
   ],
   "source": [
    "for data_item in data_list:\n",
    "    \n",
    "    image_path = data_item[0]\n",
    "#     print(image_path)\n",
    "    image_name = image_path.split('/')[-1][:-4]\n",
    "#     print(image_name)\n",
    "\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    \n",
    "    data = ([img], None)\n",
    "    \n",
    "    grid_cam = explainer_grad_cam.explain(data, model_inception, class_index=data_item[1], colormap=cv.COLORMAP_JET)\n",
    "    explainer_grad_cam.save(grid_cam, \"./images/\" + str(data_item[1])+ \"/results/\", image_name + \"-grad_cam.png\")\n",
    "        \n",
    "#     grid_occlusion = explainer_occlusion.explain(data, model_inception, class_index=data_item[1], colormap=cv.COLORMAP_JET)\n",
    "#     explainer_occlusion.save(grid_occlusion, \"./images/\" + str(data_item[1])+ \"/\", image_name + \"-occlusion_sensitivity.png\")\n",
    "\n",
    "    print(\"-----------------DONE-------------------\")\n",
    "\n",
    "\n",
    "    \n",
    "#     grid_vanilla = explainer_vanilla.explain(data_np_array, model, scorpion_class_index)\n",
    "#     explainer_vanilla.save(grid_vanilla, \".\", \"vanilla_gradients.png\")\n",
    "\n",
    "#     grid_smooth = explainer_smooth.explain(data, model, scorpion_class_index, 20, 1.0)\n",
    "#     explainer_smooth.save(grid_smooth, \".\", \"smoothgrad.png\")\n",
    "\n",
    "#     grid_grad = explainer_int_grad.explain(data, model, scorpion_class_index, n_steps=15)\n",
    "#     explainer_int_grad.save(grid_grad, \".\", \"integrated_gradients.png\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-explain",
   "language": "python",
   "name": "tf-explain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
